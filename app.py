"""
Folk Tales Question Answering System

This application provides a web interface for users to ask questions about folk tales and
receive answers generated by a fine-tuned LLaMA 3.2 model. The system loads folk tales from
a CSV file and uses a Gradio interface for interaction.

The application uses:
- Transformers library for model loading and inference
- PEFT for parameter-efficient fine-tuning
- Gradio for the web interface
- Pandas for data manipulation

Author: NLP Project Team
"""

import random
from typing import Any, Optional, Tuple

import gradio as gr
import pandas as pd
from peft import PeftModel
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)

# Path to your fine-tuned model
output_model = "llama3.2-3B-Fine-tuned-QA"
model_id = "meta-llama/Llama-3.2-3B-Instruct"  # Base model


def load_finetuned_model(model_id: str, adapter_path: str) -> Tuple[Any, Any]:
    """
    Load the fine-tuned LLaMA model with LoRA adapters.

    Args:
        model_id: The HuggingFace model ID of the base model
        adapter_path: Path to the fine-tuned LoRA adapter

    Returns:
        Tuple containing the loaded model and tokenizer
    """
    # Load base model with quantization
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype="float16",
        bnb_4bit_use_double_quant=True,
    )

    base_model = AutoModelForCausalLM.from_pretrained(
        model_id, quantization_config=bnb_config, device_map="auto"
    )

    tokenizer = AutoTokenizer.from_pretrained(adapter_path)

    # Load LoRA adapter
    model = PeftModel.from_pretrained(base_model, adapter_path)

    # Ensure model is in evaluation mode
    model.eval()

    return model, tokenizer


def load_stories() -> pd.DataFrame:
    """
    Load folk tales from the CSV file.

    Returns:
        DataFrame containing the stories with columns 'title' and 'story'
    """
    stories_df = pd.read_csv("250_stories.csv", delimiter="#")
    return stories_df


def generate_answer(
    story_title: str, question: str, progress: Optional[gr.Progress] = gr.Progress()
) -> Tuple[str, str, str]:
    """
    Generate an answer for a question based on the selected story context.

    Args:
        story_title: The title of the selected story
        question: The user's question about the story
        progress: Gradio progress tracker

    Returns:
        Tuple containing (answer_text, full_story_text, status_message)
    """
    if not story_title:
        return (
            "Please select a story first.",
            "",
            "‚ùå You must select a story before asking a question.",
        )

    if not question:
        return (
            "Please enter a question.",
            "",
            "‚ùå You must enter a question to get an answer.",
        )

    progress(0, desc="Preparing...")
    # Find the story by title
    story_context = stories_df[stories_df["title"] == story_title]["story"].values[0]

    # Format using the chat template
    messages = [
        {
            "role": "system",
            "content": "Answer the question accurately based on the given story context.",
        },
        {
            "role": "user",
            "content": f"Story name: {story_title}\n\nContext: {story_context}\n\nQuestion: {question}",
        },
    ]

    progress(0.3, desc="Processing...")
    prompt = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )

    # Tokenize and generate
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    progress(0.5, desc="Generating answer...")
    generated_ids = model.generate(**inputs, max_new_tokens=300)
    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    # Extract the model's answer from the generated text
    answer = generated_text.split("assistant\n")[-1].strip()

    progress(1.0, desc="Complete!")
    return answer, story_context, "‚úÖ Answer generated successfully!"


def show_story_preview(story_title: str) -> Tuple[str, str]:
    """
    Get a preview of the selected story.

    Args:
        story_title: The title of the selected story

    Returns:
        Tuple containing (story_preview, full_story_text)
    """
    if not story_title:
        return "", ""

    story_context = stories_df[stories_df["title"] == story_title]["story"].values[0]
    # Return a preview (first 200 characters)
    preview = story_context[:200] + "..." if len(story_context) > 200 else story_context
    return preview, story_context


def get_random_story() -> Tuple[str, str, str]:
    """
    Select a random story from the available stories.

    Returns:
        Tuple containing (story_title, story_preview, full_story_text)
    """
    random_title = random.choice(story_titles)
    story_context = stories_df[stories_df["title"] == random_title]["story"].values[0]
    preview = story_context[:200] + "..." if len(story_context) > 200 else story_context
    return random_title, preview, story_context


# Custom theme definition
theme = gr.themes.Soft(
    primary_hue="indigo",
    secondary_hue="blue",
    neutral_hue="slate",
    radius_size=gr.themes.sizes.radius_sm,
    text_size=gr.themes.sizes.text_md,
)
theme = theme.set(
    body_text_color="rgb(75, 85, 99)",
    body_background_fill="linear-gradient(to right bottom, rgb(249, 250, 251), rgb(243, 244, 246))",
    block_background_fill="white",
    block_shadow="0 1px 3px 0 rgb(0 0 0 / 0.1), 0 1px 2px -1px rgb(0 0 0 / 0.1)",
    button_primary_background_fill="rgb(79, 70, 229)",
    button_primary_background_fill_hover="rgb(67, 56, 202)",
    button_secondary_background_fill="rgb(255, 255, 255)",
    button_secondary_background_fill_hover="rgb(249, 250, 251)",
)

# Initialize application components
print("Loading the fine-tuned model...")
model, tokenizer = load_finetuned_model(model_id, output_model)
print("Model loaded successfully!")

# Load stories
print("Loading stories...")
stories_df = load_stories()
story_titles = stories_df["title"].tolist()
print(f"Loaded {len(story_titles)} stories")

# Create Gradio interface
with gr.Blocks(theme=theme, title="Folk Tales Question Answering") as demo:
    # Header
    with gr.Row():
        with gr.Column(scale=5):
            gr.Markdown(
                """
                # üßô‚Äç‚ôÇÔ∏è Folk Tales Question Answering
                
                *Ask questions about folk tales and get precise answers from our fine-tuned LLaMA 3.2 model.*
                """
            )
        with gr.Column(scale=1):
            gr.Markdown(
                "![Model](https://cdn-icons-png.flaticon.com/512/6295/6295417.png)"
            )

    with gr.Row():
        # Left column - inputs
        with gr.Column(scale=3):
            gr.Markdown("### üìö Select a Story")
            story_dropdown = gr.Dropdown(
                choices=story_titles,
                label="Choose a folk tale",
                info="Select one of our collection of folk tales",
                scale=4,
            )
            random_btn = gr.Button("üé≤ Pick Random Story", size="sm")

            with gr.Row():
                with gr.Column(scale=3):
                    story_preview = gr.Textbox(
                        label="Story Preview",
                        lines=3,
                        max_lines=3,
                        interactive=False,
                        placeholder="Story preview will appear here...",
                    )

            with gr.Group():
                gr.Markdown("### üîç Ask Your Question")
                question_input = gr.Textbox(
                    label="Your Question",
                    placeholder="e.g., What happened to the main character?",
                    lines=2,
                )
                with gr.Row():
                    submit_btn = gr.Button("üîÆ Get Answer", variant="primary")
                    clear_btn = gr.Button("üßπ Clear")

                status_message = gr.Textbox(
                    label="Status",
                    interactive=False,
                    value="Ready for your question!",
                    show_label=False,
                )

        # Right column - outputs
        with gr.Column(scale=3):
            with gr.Group():
                gr.Markdown("### ‚ú® Answer")
                answer_output = gr.Textbox(
                    label="Generated Answer",
                    lines=6,
                    max_lines=10,
                    interactive=False,
                    placeholder="The answer will appear here...",
                )

            with gr.Accordion("üìú Full Story", open=False):
                story_context = gr.Textbox(
                    label="Complete Story Text",
                    lines=12,
                    max_lines=20,
                    interactive=False,
                    placeholder="The complete story will appear here...",
                )

    # Examples section
    with gr.Accordion("üìã Example Questions", open=False):
        gr.Examples(
            examples=[
                [
                    "The Orphan Boy and the Magic Stone",
                    "How did Ayong finally resolve the haunting by the dead witches?",
                ],
                ["The Turnip", "Why did the poor brother stop being a soldier?"],
                [
                    "The Story of the Leopard, the Tortoise, and the Bush Rat",
                    "What physical feature do bush rats have because of the leopard's attack?",
                ],
            ],
            inputs=[story_dropdown, question_input],
        )

    # Add footer
    gr.Markdown(
        """
        ---
        ### About this Demo
        This demo uses a fine-tuned LLaMA 3.2-3B model to answer questions about folk tales.
        The model was trained on a dataset of folk tales with question-answer pairs.
        
        *Created as part of NLP Assignment on Question Answering Systems*
        """
    )

    # Set up the event handlers
    submit_btn.click(
        fn=generate_answer,
        inputs=[story_dropdown, question_input],
        outputs=[answer_output, story_context, status_message],
    )

    story_dropdown.change(
        fn=show_story_preview,
        inputs=[story_dropdown],
        outputs=[story_preview, story_context],
    )

    random_btn.click(
        fn=get_random_story,
        outputs=[story_dropdown, story_preview, story_context],
    )

    clear_btn.click(
        fn=lambda: ("", ""),
        outputs=[question_input, answer_output],
    )

if __name__ == "__main__":
    # Launch the Gradio web interface
    demo.launch(share=True)
